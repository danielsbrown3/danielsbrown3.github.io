<!DOCTYPE html>
<html>

  <head>
<link href='./style.css' rel = 'stylesheet'>
    
  </head>
<body>
<h1 class='title'>Daniel Brown</h1>
<p>I'm hosted with Github Pages. Hi I'm Daniel and I Love to read. This website will document the thoughts 
  and feelings I have about current books I am reading.</p>
<h2 class='title'>Books Currently Reading</h2>
  <h3 class='title'> <em>A Thousand Brains by</em> Jeff Hawkins </h3>
  <p> I'm currently reading <em>A Thousand Brains</em> by Jeff Hawkins about the neuroscience behind the brain and how that informs technology.
    The introduction of the book starts as <span>Jeff Hawkins</span> lays the groundwork of his career. <br> <br> His career is very inspiring, he started as an aspiring
    neuroscientist student who was denied his <strong>PHD</strong> at Berkeley. After being denied he created his own venture into neurology research and shortly after went
    into the computing industry in silicon valley. He founded Palm Computing which produced the first computing tablet. He shortly after continued his journey into 
    neurology by founding the company Numenta. <br> 
    <b>Part 1  - "A New Understanding of the Brain"</b>
    Defines what intelligence is and how incredible it is that a collection of cells in our brain can make models of the world and consistently update those models.
    Intelligence is the ability to create and update models in your brain. The other then goes on to decscibe that there are two portions of the brain, the repitlian
     "old brain" and the Neocortex. The reptilian old brain is responsible for keeping us alive and reproducing, preserving our genes. The Neocortex, a sheet of cells
    that wraps fully around the brain for miles at a time. Jeff also describes his life story as to provide an ethos for the reader on why to trust the claims he is 
    making. His life is quite incredible, he started working at the early days of Intel, then pursued neuroscience research, was rejected by every big university, studied
    on his own for 2 years, started a computer company Palm Computing, was very successful, resumed his neorology research with Numenta. <br>
    <b> Chapter 1 - "Old Brain-New Brain"</b>
    A more in depth view of the old brain and Neocortex. The Neocortex cannot control movement, only the old brain can. The Neocortex must overcome the urges of the old
    brain in order to complete any actions. This is why it is difficult for humans to resist sex, high sugar and fat foods, and things that will help reproduce. The Neocortex
    is structured like a computer, control boxes with input and outputs leading to different control boxes. There are sections of the brain that are dedicated to vision, smell
    etc. However, the author argues that even though there are sections of the brain dedicated towards those functions, the cells are not built specifically for that 
    function. All brain cells are built the same, to create models. For example, there was a study conducted on a frog where scientists added a third eye to the frogs
    head. Instead of the vision sector of the brain supporting those cells, cells from all other sensing parts of the frogs brain supported the eye. Therefore any part
    of the brain can be used for any sensation, because they are all just model cells. <br> 
    <b>Chapter 2 - "Vernon Mountcastle's Big Idea"</b>
    Neocortex did not evolve to produce cells for a specific task, rather it just kept producing more and more of the same brain cells and found uses for them. Hawkins
    unveils one of his theories: cortical columns (long columns of brain cells interconnected in the brain) are algortihms as evident by similair circuits, evolved in a short amount of time, 
    can be adapted to any sensory input, and the fact that humans are capable of so many unrelated task. <br> 
    <b>Chapter 3- "A Model of the World in Your Head"</b>
    Neocortex works by modeling -> predicting -> checking error. The brain is constantly running a feedback loop that responds to the changing world and us moving 
    ourselves. To learn you must get error and create new model. The numnber of neurons in your brain that are active at the same time is around 2% (maybe I shouldn't
    multitask since I only have 2% avaialbe to me). Synaspes in the brain that store memories or "models" are strengthened or weakened, rather they are created or
    destroyed.  <br>
    <b>Chapter 4- "The Brain Reveal Its Secrets"</b> 
    Anything can be learned if humans have learned it. No predictor neurons, when holding a cup we are not aware of the cup. Our brain has to predict the world and then
    predict how it can manipulate it. 90% of synapses, the small little hairs (not hairs but looks like it) along the neurons that recieve input, are located on dendrites.
    Dendrites are the roots that expand out of the neuron and connect to other neurons. 
    IMPORTANT- In this chapter it desribes how electrical signals determine how we learn, I think this is one of the most important ideas in the book. Page 46.
    Some neurons in the brain, already have a model for something we've learned before. Once the brain recieves that similair input, the neurons that have that model will
    fire first and all other neurons will not fire. However, if there is something that you have not learned before, or seen before, there is no neurons that already have
    that model stored. So all neurons will fire at the same time on what model they think that new thing is. The model that gets the most neurons firing on the same thing
    is the model that is accepted. <br>
    <b>Chapter 5- "Maps in the Brain"</b> 
    I enjoyed this chapter because it addresses an idea that I haven't put much thought into: perception. How does our brain keep
    track of everything in the physical world? How does it keep your organs operational while creating a model of the enviorment 
    around me while still being able to cognitivelly think? An earth worm does not know the enviorment around it, rather it senses
    temparture and humidity and moves towards it. Such a simple computational algorithm, yet earth worms have survived millions of 
    years of evolution. The author continues on to describe grid cells and place cells, which are a little conceptially difficult for
    me to understand, but I'll try my best. From what I understood, place cells are cells that create the model of what you observe
    using sensory inputs, whereas grid cells are cells that control your perception of where you are with respect to the place cells
    model. For example, if I'm at a Chiptole, my place cells would represent the counter, bathroom, the sleek metal counter, the aroma
    of chicken, and my grid cells would be where my body is with respect to the chipotle my heart is in my chest my brain is in my 
    head etc. <br>
    <b>Chapter 6- "Concepts, Language, and High-Level Thinking"</b>
    High level thinking occurs in the same cells as sensory input. How do we make models for things we cannot see? How can I make a
    model for democracy? That is not something I can physically see in the world. Due to this conutrum, we can conclude that abstract
    thoughts that are not physical models are also stored in the same cells that physical models are. There is no difference in the cells
    that store each of the ideas. The brain is very organized with grid cells and place cells and cortical columns. Thinking in the brain
    are neurons sorting through reference frames (the frames produced by creating a model with place and grid cells). It is possible
    to store new thoughts on old reference frames, but that is usually not how it works. New ones are usually created and old ones deleted
    The brain moves linear to create a model just as your brain moves linearly to remember what the inside of your house looks like. 
    Thinking is just moving through neurons in your brain, when you've thought of something before, your brain stores that at a coordinate
    in your brain, and once you think of it again it begins moving at that coordinate. When we don't know a coordinate to start at we 
    become confused. Our brain hyperlinks to other parts of the brain. Reptile brain creates maps of enviorments. Neocortex describes
    what and where objects are in those enviorments and also creates maps of concepts. Its not about having a big brain, its about organizing
    the thoughts in your brain that matters. <br>
    <b>Chapter 7- "The Thousand Brains Theory of Intelligence"</b>
    The brain works like the Ford assembly line, in order for the eyes to "see" something it goes through multiple stages. Stage 1 could
    consist of seeing lines. Stage 2 could be responsible for corners. Stage 3 could be responsible for creating the whole image etc.
    The world our eyes see is not the perception of the world. Its probably 70% of the world we actually see. The other 30% is filled in 
    by our brain. Its not the size of the camera that matters, its the size of the processor. Knowledge is neither stored everywhere or in 
    a single cortical column. It is distributed amongst large amounts of cortical columns in clusters. Since knowledge is distributed 
    amongnst thousands of cortical columns, how do we see a single perception? The author hypothesizes that the columns vote. Process voting occurs.
    The best way I can explain this is with an analogy the author uses, a hand holding a coffee cup. Using one finer, you might be able to determine
    what the coffee cup is, but it will take a lot of work and time. However with 5 fingers, each one can vote on what it thinks the coffee cup
    is and the majority will most likely be right. Seizures occur when the brain cannot decide what the majority is and it tries to do every
    single thing at once. This is why doctors cut corticle column connections in the brain to stop seizures. Multi tasking is not good! Can only
    compute one model at once. <br>
    <b>Part 2- "Machine Intelligence"</b>
    Artificial Intelligence will be based off of the construction of the brain. Evolution is the best designed so trying to emulate
    nature in computes is inherently the best design. 
    <b>Chapter 8- "Why There Is Ni "I" in AI"</b>
    Machines are trained currently, they are not intelligent. They learn based on labeled data, on a very specific task. If you train
    a car on self driving, you can then ask it to fry and egg. It is not intelligence rather it is memorizing a pattern to a specific 
    subset of data and can't apply it to other subsets of data. Deep learning is not what the brain does currently, rather it is a pattern
    recognition algorithm that cannot adapt its patterns across categories. Knowledge representation is the leading problem for artificial
    intelligence. LLM's are great advancements in technology, but they are still fundamentally flawed compared to the Neocortex. Until 
    LLMs can learn models and not just a bunch of rules, they will not reach AGI. AI will develop as computers have. First computers, were
    very large and slow and specified for very specific tasks. Then as the price goes down, computers became to work on a lot of the same
    principles and universal computers that are able to network with other computers, compile code, etc are now the norm. The same
    tradjectory will follow with machine learning. The most important component of how the brain works is the neuron. When the neuron
    learns something new, it stores it on a synapse  that does not effect other synapses nearby. I don't understand why we cant just 
    cluster a bunch of machine learning devices together just like the brain clusters synapses together. Hawkins main argument is that until
    machines can learn continuously with deleting memory, learn via sensory input, abd stire tens of thousands of models it will not rival 
    the brain and it is not AGI. <br>
    <b>Chapter 9 - "When Machines Are Consicous"</b>
    What is consciousness? The author argues that consciosness is dependant entirely on memory. If you did not remember something were
    you even conscious at all? Not to the brain you weren't. "If we couldn't replay our recent thoughts and expierences, then we would be
    unaware that we are alive." Qualia is the phenonema of how sensory inputs are percieved. For example, as humans we can probably all agree
    on the identity of the colors that we see. However, we don't know if that color that we are seeing is the same as the one everyone else is.
    We know different wavelengths produce different images, but we don't know if that image is the same for everyone. My blue might be 
    different than someone else's blue. Qualia of color is not constant. The author discusses how attention leads us to believe we are
    conscious, but I did not follow. The author then argues that AI will not be harmful because it does not have the reptile brain and 
    it cannot reproduce. Since it does not need to preserve its genes it does not fear death or seek to harm rivals. Therefore AI will
    not be dangerous. Unless we specifically make it dangerous. <br>
    <b> Chapter 10- "The Future of Machine Intelligence" </b>
    The boundares of the brain are the boundaries of AI (I see where the author is coming from, but you can definetly scale the brain 
    more with silicon than trying to contain it within a skull). Artificial intelligence needs to be able to explore on its own, whether
    it be exploring the internet, or using sensors to explore the physical world. In order for it to become AGI, it needs to able to explore
    and learn on its own. Artifical intelligence will not be the end all be all for innovation. There is still a bottleneck that will occur
    with artificial intelligence as there is with humans. Scientific discoverey is done by testing expierements in a lab, an artificial 
    intelligent system may be able to speed up simulations of an expierement, but it will not be able to speed up the physical expierement.
    It can't force bacteria to grow quicker or viruses to spread faster. It will be controlled with respect to time by a lot of the time
    limitations humans are controlled by. Machines can learn optimally, transfer mass amount of data optimally, but cannot speed up 
    expieremental data. <br>
    <b> Chapter 11- "The Existential Risks of Machine Intelligence" </b>
    At the beginning of the 21rst century, AI was a curse word in industry. The talk around intelligent machines, usually devolves
    around bad people using intellgent machines. There is a bottle neck in machine learning, although machines may be able run a million
    times faster than a biological brain, they cannot acquire new knowledge a million times faster. I disagree with the author on a few 
    points made in this chapter. The first being the argument stated above that the bottle neck is ataining knowledge, and while it 
    is true that machines can not run expierements faster than humans, they can simulate expierements much faster and reduce the
    amount of expierementation that needs to occur to discover something new. The second thing I disagree to an extent with is a
    counterexample to the dangers of AI being that things that are not intelligent kill the most humans. Virus, bacteria and disease
    are seen as not very intelligent creatures that kill more humans than any other intelligent creature. I feel as if this is a 
    cop out, its not about whether or not intelligence is the sole factor in increasing danger to human beings. Rather it is the
    efficiency and scale machines accomplish things in. The author also states something that I found super interesting in the chapter.
    The overall argument that self replication is dangerous and not intelligence is a good argument, but I believe there a just a few
    flaws, as stated above. However, it is true that it is the preservation of genes that causes life to harm other life. Greed and war
    both stem from the desire to preserve genes. <br>

    <b> Part 3 - Human Intelligence</b>
    Reproduction has caused extiction and death but intelligence has caused the opposite and improved lives. This is the 
    primary argument of the author. <br>

    <b> Chapter 12 - False Beliefs</b>
    "We live in a simulation". The author takes an interesting take on this well discussed topic, by stating that we do indeed
    live in a simulation as our brain constantly interpolates what we see around us. If we had different sensors, then the simulation
    we live in would be different. A frog sees the world much differently than I do. Expierences, like humans experiencing ghost 
    limb, leads us to conclude that the simulation we see can sometimes be wrong. An aside from me, what if there is aliens that 
    have much better biological sensors and they can like see the world better than us. Like their eye sensors can zoom down to 
    atomic levels. That would be pretty cool. We are constantly adjusting our model of the world. Viral models of the world, are
    models that spread due to the ability to increase the probabilty of passing on genes. For example, education is a viral model
    people who get a good education will get better access to financial resources and health care and will have a better probability
    of passing on their genes. The history book may be factually incorrect, but life is not about having a correct model, its about 
    having the model that will propagate genes. There is only one way to discern falsehoods from truths, it is to actively seek 
    contradictions to our beliefs. <br>

    <b> Chapter 13- The Existential Risks of Human Intelligene</b>
    Technological advancements in the last 10000 years have exponentially increased, however our brain chemistry has remained 
    relatively the same. Our reptile brain loves sugar, even though it does not support our health. The brain has not evolved
    to match the speed of sugar production. The reptile brain usually wins the fight when their is a dispute in choosing an 
    action to complete. However, there are more and more instances of the Neocortex beating the old brain occuring as intelligence
    grows. For example, a large problem our societies face is overpopulation, contrary to billionaires perspectives who want cheap 
    labor, the more people the less resources are available per person. An effective method of population growth has been birth 
    control. This has been a great solution to population control. Three factors lead to the propagation of false beliefs: 
    1. Cannot directly expierence: can't sense it, can't easily disprove to large number of people<br>
    2. Ignore contrary evidence: dismiss evidence that contradicts it <br>
    3. Viral spread: encourage behaviors that spread to other people <br>
    I'm glad the author addresses some of the most controversial topics our society faces today. He approaches it from an 
    evolutionary and genetic point of view, which is extremely refreshing. <br>

    <b> Chapter 14 - Merging Brains and Machines </b>
    
    


    <br>
    <b>Goals</b>
    <li>Gain the ability to explain a topic using a story</li>
    <li>Keep someone capitivated in a story</li>
    <li>Be comfortable solving problems using programming languages</li>
    <li>Approach technical problems from a problem and solution point of view</li>
    <li>Confront someone without berading them</li>
    <li>Have an uncomfortable conversation</li>
    <li>Try creating machine learning problem and fail</li>
    <li>Be able to understand Kaggle, medium, and cutting edge ML research</li>
    <li>Explain how a computer works from silicon to user interface</li>
    <li>Change my role to data science at work</li>
    <li>Lead a project</li>
    <li>Post a youtube video</li>
    <li>Program everyday</li>
    <li>Continue to work on something even if its not novel</li>
    <li>Resist temptation (sugar)</li>
    <li>Approach everyone in life with a goal to help them or learn about them</li>
    <li>Listen intently when someone speaks and communicate that I am listening </li>
    <li>Call everyone by there first name at least once everytime I see them</li>
    <li>Focus 100% on the task I am currently doing, no coasting</li>
    
    
    
    

    
    <b>My first machine learning code 8/5/2023:</b>
    I created my first code! With a step by step tutorial of course, so hopefully I can adapt it to be my own soon. However, to make sure my dendrites create an 
    accurate model of what I learned, I'm going to try and teach it to myself here.  <br>
    First lets describe what a machine learning model really is: <br>
    <li>1. Recieves data as input</li>
    <li>2. Manipulates the inputs in a hidden layer by applying weights and bias</li>
    <li>3. Spits out an output as a result.</li>
    <li>4. Assesses the error of the output compared with the initial data.</li>
    <li>5. Uses an algorithms to reduce the error and iterate.</li> <br>
    Now that we know an overview, lets discuss how to accomplish it. First, we want to push the data through our layers and get an output using Forward Propagation.
    y = w *(dot product) x + b. Where y is the output, x is the input, w are the randomly generated weights, and b is the bias. y,x, and w are arrays and b is a scalar.
    Once we have our output y from applying our weights and bias, we want to compute the error, dE/dy. The error can be computed using the Mean Squared Error equation: 
    dE/dy = 2 / (total number of outputs) * [correct output[i] - predicted output[i]]. Where i corresponds to which element you are computing the error of. One you have DE/dy
    you can begin Back Propagation. Where you start from the output and work your way backwards computing error such that you can accomplish gradient descent and change 
    the weights and biases to reduce error. To accomplish back propagation, need to also computer dE/dx (change in error with respect to x), dE/dB(change in error with 
    respect to bias), and dE/dw(change in error with respect to weights. The derviation of each of these values involves the chain rule, dE/dx = dE/dy * dy/dx. 
    dE/dB = dE/dy * dy/dB. dE/dw = dE/dy * dy/dw. Where the dy/d(insert) derivation is found from the equation y = w * x + b. The neural network also needs an activation
    layer which basically a nonlinear componenet to make the model more accurate. The nonlinear activation layer I used was tanh(x), for this layer you do element wise 
    multiplication. (I am a little unsure about this part as I'm not sure if this activation layer is a completely different model or apart of the current linear model)
    ->Research activation layer and nonlinear models<-
  </p>
</body>
</html>
